{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy\n",
    "spaCy is an open-source software library for advanced Natural Language Processing, written in the programming languages Python and Cython.Unlike NLTK, which is widely used for teaching and research, spaCy focuses on providing software for production usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TOKENIZATION\n",
    "One common task in NLP (Natural Language Processing) is tokenization. \"Tokens\" are usually individual words (at least in languages like English) and \"tokenization\" is taking a text or set of text and breaking it up into its individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HI\n",
      "My\n",
      "name\n",
      "is\n",
      "Nikita\n",
      ".\n",
      "I\n",
      "will\n",
      "teach\n",
      "you\n",
      "NLP\n"
     ]
    }
   ],
   "source": [
    "nlp\n",
    "\n",
    "example1 = nlp(\"HI My name is Nikita.I will teach you NLP \")\n",
    "for element in example1 :\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "quick\n",
      "brown\n",
      "fox\n",
      "jumps\n",
      "over\n",
      "the\n",
      "lazy\n",
      "dog\n"
     ]
    }
   ],
   "source": [
    "example2 = nlp(\"The quick brown fox jumps over the lazy dog\")\n",
    "\n",
    "for token in example2:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEMMING\n",
    "Stemming is the process of reducing a word to its word stem \n",
    "that affixes to suffixes and prefixes or to the roots of words known\n",
    "as a lemma. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## from nltk import stem\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat', 'run', 'wa']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = \"Cats Running Was\"\n",
    "[stemmer.stem(token) for token in example.split(' ')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### above example it has splited the data in its core form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you',\n",
       " 'better',\n",
       " 'lose',\n",
       " 'yourself',\n",
       " 'in',\n",
       " 'the',\n",
       " 'music,',\n",
       " 'the',\n",
       " 'moment',\n",
       " 'you',\n",
       " 'own',\n",
       " 'it,',\n",
       " 'you',\n",
       " 'better',\n",
       " 'never',\n",
       " 'let',\n",
       " 'it',\n",
       " 'go',\n",
       " 'you',\n",
       " 'onli',\n",
       " 'get',\n",
       " 'one',\n",
       " 'shot,',\n",
       " 'do',\n",
       " 'not',\n",
       " 'miss',\n",
       " 'your',\n",
       " 'chanc',\n",
       " 'to',\n",
       " 'blow',\n",
       " 'thi',\n",
       " 'opportun',\n",
       " 'come',\n",
       " 'onc',\n",
       " 'in',\n",
       " 'a',\n",
       " 'lifetim',\n",
       " '']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example2 = \"You better lose yourself in the music, the moment \"\\\n",
    "+ \"You own it, you better never let it go \"\\\n",
    "+ \"You only get one shot, do not miss your chance to blow \"\\\n",
    "+ \"This opportunity comes once in a lifetime \"\n",
    "[stemmer.stem(token) for token in example2.split(' ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y o u   b e t t e r   l o s e   y o u r s e l f   i n   t h e   m u s i c ,   t h e   m o m e n t   Y o u   o w n   i t ,   y o u   b e t t e r   n e v e r   l e t   i t   g o   Y o u   o n l y   g e t   o n e   s h o t ,   d o   n o t   m i s s   y o u r   c h a n c e   t o   b l o w   T h i s   o p p o r t u n i t y   c o m e s   o n c e   i n   a   l i f e t i m e  \n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(example2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "be\n",
      "be\n",
      "be\n"
     ]
    }
   ],
   "source": [
    "example = nlp(\"is am are\")\n",
    "for token in example:\n",
    "    print(token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-PRON-\n",
      "better\n",
      "lose\n",
      "-PRON-\n",
      "in\n",
      "the\n",
      "music\n",
      ",\n",
      "the\n",
      "moment\n",
      "-PRON-\n",
      "own\n",
      "-PRON-\n",
      ",\n",
      "-PRON-\n",
      "better\n",
      "never\n",
      "let\n",
      "-PRON-\n",
      "go\n",
      "-PRON-\n",
      "only\n",
      "get\n",
      "one\n",
      "shot\n",
      ",\n",
      "do\n",
      "not\n",
      "miss\n",
      "-PRON-\n",
      "chance\n",
      "to\n",
      "blow\n",
      "this\n",
      "opportunity\n",
      "come\n",
      "once\n",
      "in\n",
      "a\n",
      "lifetime\n"
     ]
    }
   ],
   "source": [
    "example2 = \"You better lose yourself in the music, the moment \"\\\n",
    "+ \"You own it, you better never let it go \" \\\n",
    "+ \"You only get one shot, do not miss your chance to blow \"\\\n",
    "+ \"This opportunity comes once in a lifetime\"\n",
    "\n",
    "example3 = nlp(example2)\n",
    "\n",
    "for token in example3:\n",
    "    print(token.lemma_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VECTORIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of converting NLP text into numbers is called vectorization in ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True, token_pattern=r'\\b[^\\d\\W]+\\b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 1 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "corpus = [\"The dog is on the table\", \"the cats now are on the table\"]\n",
    "vectorizer.fit(corpus)\n",
    "print(vectorizer.transform([\"The dog is on the table\"]).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "are: 0\n",
      "cats: 1\n",
      "dog: 2\n",
      "is: 3\n",
      "now: 4\n",
      "on: 5\n",
      "table: 6\n",
      "the: 7\n"
     ]
    }
   ],
   "source": [
    "vocab = vectorizer.vocabulary_\n",
    "\n",
    "for key in sorted(vocab.keys()):\n",
    "    print(\"{}: {}\".format(key, vocab[key]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='\\\\b[^\\\\d\\\\W]+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus2 = [\"I am jack\", \"You are john\", \"I am john\"]\n",
    "vectorizer.fit(corpus2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 1 0 0]\n",
      " [0 1 0 0 1 1]\n",
      " [1 0 1 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.transform(corpus2).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am: 0\n",
      "are: 1\n",
      "i: 2\n",
      "jack: 3\n",
      "john: 4\n",
      "you: 5\n"
     ]
    }
   ],
   "source": [
    "vocab = vectorizer.vocabulary_\n",
    "\n",
    "for key in sorted(vocab.keys()):\n",
    "    print(\"{}: {}\".format(key, vocab[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Embedding (word2vec) references\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man man 1.0\n",
      "man woman 0.7401744\n",
      "man king 0.40884617\n",
      "man queen 0.27109137\n",
      "woman man 0.7401744\n",
      "woman woman 1.0\n",
      "woman king 0.26556596\n",
      "woman queen 0.4066065\n",
      "king man 0.40884617\n",
      "king woman 0.26556596\n",
      "king king 1.0\n",
      "king queen 0.72526103\n",
      "queen man 0.27109137\n",
      "queen woman 0.4066065\n",
      "queen king 0.72526103\n",
      "queen queen 1.0\n"
     ]
    }
   ],
   "source": [
    "example1 = \"man woman king queen\"\n",
    "tokens = nlp(example1)\n",
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table table 0.29582644\n",
      "table chair 0.3401051\n",
      "table book 0.16623081\n",
      "table pen 0.12119902\n",
      "table pencil 0.13445242\n",
      "chair table 0.29582644\n",
      "chair chair 0.3401051\n",
      "chair book 0.16623081\n",
      "chair pen 0.12119902\n",
      "chair pencil 0.13445242\n",
      "book table 0.29582644\n",
      "book chair 0.3401051\n",
      "book book 0.16623081\n",
      "book pen 0.12119902\n",
      "book pencil 0.13445242\n",
      "pen table 0.29582644\n",
      "pen chair 0.3401051\n",
      "pen book 0.16623081\n",
      "pen pen 0.12119902\n",
      "pen pencil 0.13445242\n",
      "pencil table 0.29582644\n",
      "pencil chair 0.3401051\n",
      "pencil book 0.16623081\n",
      "pencil pen 0.12119902\n",
      "pencil pencil 0.13445242\n"
     ]
    }
   ],
   "source": [
    "example2 ='table chair book pen pencil';\n",
    "example2 = nlp(example2)\n",
    "for token3 in example2:\n",
    "    for token4 in example2:\n",
    "        print(token3.text , token4.text ,token1.similarity(token4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spain russia 0.57819444\n",
      "spain madrid 0.71929747\n",
      "spain moscow 0.5162205\n",
      "russia spain 0.57819444\n",
      "russia madrid 0.43594515\n",
      "russia moscow 0.7492537\n",
      "madrid spain 0.71929747\n",
      "madrid russia 0.43594515\n",
      "madrid moscow 0.5473875\n",
      "moscow spain 0.5162205\n",
      "moscow russia 0.7492537\n",
      "moscow madrid 0.5473875\n"
     ]
    }
   ],
   "source": [
    "example1 = \"spain russia madrid moscow\"\n",
    "tokens = nlp(example1)\n",
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        if(token1.text == token2.text):\n",
    "            continue\n",
    "        print(token1.text, token2.text, token1.similarity(token2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat dog 0.80168545\n",
      "dog cat 0.80168545\n"
     ]
    }
   ],
   "source": [
    "example1 = \"cat dog\"\n",
    "tokens = nlp(example1)\n",
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        if(token1.text == token2.text):\n",
    "            continue        \n",
    "        print(token1.text, token2.text, token1.similarity(token2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google ORG\n",
      "Larry Page PERSON\n",
      "Sergey Brin ORG\n",
      "the United States of America GPE\n",
      "one CARDINAL\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "example = \"Google, a company founded by Larry Page and Sergey Brin in the United States of America \"\\\n",
    "+ \"has one of the worldâ€™s most advanced search engines.\"\n",
    "\n",
    "doc = nlp(example)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U.S. GPE\n",
      "Taliban ORG\n",
      "America GPE\n",
      "three CARDINAL\n",
      "NBC News ORG\n"
     ]
    }
   ],
   "source": [
    "example = \"U.S. officials are meeting with former Taliban members \"\\\n",
    "+ \"amid intensifying efforts to wind down America's longest war, three of the \"\\\n",
    "+ \"militant group's commanders told NBC News.\"\n",
    "\n",
    "doc = nlp(example)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
